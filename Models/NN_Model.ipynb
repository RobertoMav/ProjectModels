{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec29bd8a",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348cfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b2e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b9c3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Functions_Modularity as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48d28ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>AgeFare</th>\n",
       "      <th>SibPar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2708.7654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>206.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1858.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>281.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Q  S    AgeFare  SibPar\n",
       "0         0       3    1  22.0      1      0   7.2500  0  1   159.5000       1\n",
       "1         1       1    0  38.0      1      0  71.2833  0  0  2708.7654       1\n",
       "2         1       3    0  26.0      0      0   7.9250  0  1   206.0500       0\n",
       "3         1       1    0  35.0      1      0  53.1000  0  1  1858.5000       1\n",
       "4         0       3    1  35.0      0      0   8.0500  0  1   281.7500       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv(\"../TrainTestSet/TrainSet1.csv\")\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b1781",
   "metadata": {},
   "source": [
    "### Creating x_train, y_train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_set['Survived']\n",
    "x = train_set.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42fc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af203c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efd565bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d453d6",
   "metadata": {},
   "source": [
    "### Creating a TF-Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77fac65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the sigmoid activation function to get the best weights and biases for the created model (below)\n",
    "model_NN = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(10,)),\n",
    "        Dense(units=32, activation='sigmoid'),\n",
    "        Dense(units=16, activation='sigmoid'),\n",
    "        Dense(units=1, activation='sigmoid'),\n",
    "    ], name=\"model_NN\")\n",
    "\n",
    "model_NN.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b8851",
   "metadata": {},
   "source": [
    "#### Using an early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a63fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5eca1c",
   "metadata": {},
   "source": [
    "### Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5e62d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.6577 - val_loss: 0.6619\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6494 - val_loss: 0.6528\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6414 - val_loss: 0.6425\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6321 - val_loss: 0.6310\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6198 - val_loss: 0.6156\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5997\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5929 - val_loss: 0.5813\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5756 - val_loss: 0.5629\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5609 - val_loss: 0.5445\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5468 - val_loss: 0.5306\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5325 - val_loss: 0.5131\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5197 - val_loss: 0.4996\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5091 - val_loss: 0.4869\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4994 - val_loss: 0.4772\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4907 - val_loss: 0.4660\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4832 - val_loss: 0.4580\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4773 - val_loss: 0.4512\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4720 - val_loss: 0.4464\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4683 - val_loss: 0.4423\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4655 - val_loss: 0.4378\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4627 - val_loss: 0.4339\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4603 - val_loss: 0.4304\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4592 - val_loss: 0.4289\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4582 - val_loss: 0.4271\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4265\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4558 - val_loss: 0.4254\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4549 - val_loss: 0.4254\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4540 - val_loss: 0.4235\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4243\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4526 - val_loss: 0.4219\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4522 - val_loss: 0.4208\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4518 - val_loss: 0.4207\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.4205\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4510 - val_loss: 0.4205\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4505 - val_loss: 0.4191\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4186\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4197\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4498 - val_loss: 0.4165\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4162\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4496 - val_loss: 0.4181\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.4156\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4482 - val_loss: 0.4178\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4187\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.4170\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4472 - val_loss: 0.4154\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4475 - val_loss: 0.4159\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.4179\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.4147\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4460 - val_loss: 0.4150\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4146\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4166\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4154\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4455 - val_loss: 0.4145\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4450 - val_loss: 0.4144\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4445 - val_loss: 0.4139\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4128\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.4136\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4126\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4437 - val_loss: 0.4131\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4433 - val_loss: 0.4130\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4430 - val_loss: 0.4122\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4441 - val_loss: 0.4152\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4437 - val_loss: 0.4122\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4435 - val_loss: 0.4151\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4423 - val_loss: 0.4115\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4126\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4418 - val_loss: 0.4142\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4413 - val_loss: 0.4133\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4122\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4414 - val_loss: 0.4121\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4126\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4139\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4109\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4412 - val_loss: 0.4121\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4103\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4117\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4090\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4396 - val_loss: 0.4122\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4100\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4397 - val_loss: 0.4086\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4392 - val_loss: 0.4113\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4092\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.4095\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4082\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4385 - val_loss: 0.4089\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4102\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4374 - val_loss: 0.4102\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4101\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4093\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4097\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4097\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4366 - val_loss: 0.4083\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.4088\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4092\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4364 - val_loss: 0.4077\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4075\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4057\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4351 - val_loss: 0.4089\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4359 - val_loss: 0.4074\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4353 - val_loss: 0.4070\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4354 - val_loss: 0.4057\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4081\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4070\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4054\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4043\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4340 - val_loss: 0.4060\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4073\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4072\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4055\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4051\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4050\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4031\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4330 - val_loss: 0.4047\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4040\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4043\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4339 - val_loss: 0.4037\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4321 - val_loss: 0.4042\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4038\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4028\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4042\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4329 - val_loss: 0.4038\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4021\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4028\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4038\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4013\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4009\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4305 - val_loss: 0.4025\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4024\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4018\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4009\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4310 - val_loss: 0.4035\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4313 - val_loss: 0.4027\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4304 - val_loss: 0.4015\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4305 - val_loss: 0.4012\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4026\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4304 - val_loss: 0.4041\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4010\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.3999\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.3997\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4006\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.3997\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.3991\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.4007\n",
      "Epoch 144/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.4006\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.3988\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4014\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4296 - val_loss: 0.4008\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.3989\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4014\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.3998\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.3972\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4282 - val_loss: 0.3974\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.3974\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.3982\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3974\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.3972\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.3964\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.3965\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3958\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.3974\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.3958\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.3963\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3979\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4267 - val_loss: 0.3964\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3966\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4260 - val_loss: 0.3953\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.3956\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4260 - val_loss: 0.3951\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.3949\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.3947\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4260 - val_loss: 0.3955\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4258 - val_loss: 0.3936\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.3947\n",
      "Epoch 174/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.3949\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.3953\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.3952\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.3944\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.3946\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4249 - val_loss: 0.3938\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4247 - val_loss: 0.3938\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3943\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4250 - val_loss: 0.3936\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.3930\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3923\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4240 - val_loss: 0.3930\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.3918\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.3920\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.3938\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.3913\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3899\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3919\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.3906\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.3922\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.3905\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.3903\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4234 - val_loss: 0.3922\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.3905\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.3919\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.3897\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.3891\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.3894\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.3895\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.3890\n",
      "Epoch 204/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.3910\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.3906\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.3901\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4211 - val_loss: 0.3879\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.3890\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4213 - val_loss: 0.3885\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4219 - val_loss: 0.3884\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.3888\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.3882\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.3896\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.3873\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.3879\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3866\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4204 - val_loss: 0.3875\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.3876\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3868\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.3887\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4199 - val_loss: 0.3874\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4197 - val_loss: 0.3891\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.3867\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.3859\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3853\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.3849\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.3831\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4193 - val_loss: 0.3838\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.3835\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.3840\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.3841\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.3860\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4198 - val_loss: 0.3871\n",
      "Epoch 234/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4182 - val_loss: 0.3847\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.3851\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.3857\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.3838\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3855\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3844\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3854\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4173 - val_loss: 0.3835\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3827\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4179 - val_loss: 0.3841\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4171 - val_loss: 0.3826\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.3831\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 0.3839\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.3817\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3838\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4168 - val_loss: 0.3848\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3835\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3832\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.3823\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.3823\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4156 - val_loss: 0.3832\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3826\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.3803\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4159 - val_loss: 0.3812\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4152 - val_loss: 0.3809\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4149 - val_loss: 0.3786\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.3796\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.3788\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3800\n",
      "Epoch 263/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.3830\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3837\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3801\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.3812\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.3814\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.3788\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4136 - val_loss: 0.3817\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3810\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4139 - val_loss: 0.3798\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.3804\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3794\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.3785\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.3801\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3792\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4134 - val_loss: 0.3806\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3781\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.3777\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.3798\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3789\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4121 - val_loss: 0.3783\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.3787\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3800\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4123 - val_loss: 0.3777\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3816\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3806\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3773\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.3786\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4112 - val_loss: 0.3790\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4108 - val_loss: 0.3795\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.3798\n",
      "Epoch 293/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.3792\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3768\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.3786\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4099 - val_loss: 0.3772\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4101 - val_loss: 0.3768\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.3765\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.3776\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.3801\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4093 - val_loss: 0.3781\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.3790\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.3781\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4132 - val_loss: 0.3796\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4092 - val_loss: 0.3765\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.3769\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.3758\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4087 - val_loss: 0.3766\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.3764\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.3770\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.3771\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.3770\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.3784\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4078 - val_loss: 0.3771\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.3783\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.3792\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.3761\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.3751\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4079 - val_loss: 0.3761\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.3759\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.3765\n",
      "Epoch 322/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3771\n",
      "Epoch 323/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.3744\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.3728\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3740\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.3744\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4055 - val_loss: 0.3748\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3742\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.3749\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.3759\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3756\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.3757\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.3739\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.3747\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4051 - val_loss: 0.3760\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4057 - val_loss: 0.3740\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4045 - val_loss: 0.3729\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4053 - val_loss: 0.3745\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.3744\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.3740\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4055 - val_loss: 0.3771\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.3744\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.3744\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3738\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.3735\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4034 - val_loss: 0.3743\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.3747\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4037 - val_loss: 0.3743\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4035 - val_loss: 0.3734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24a82354d00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model\n",
    "model_NN.fit(x=X_train, y=y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b0b0f",
   "metadata": {},
   "source": [
    "### Plotting the loss metrics vs epoch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e92aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_mol = pd.DataFrame(model_NN.history.history)\n",
    "loss_mol.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2753d203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "[[0.984075]]\n"
     ]
    }
   ],
   "source": [
    "#Checking the 1st train_set prediction (y = 0)\n",
    "print(model_NN.predict(X_train[0].reshape(1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c68a58",
   "metadata": {},
   "source": [
    "### Obtaining the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3996aabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 601us/step - loss: 0.3734\n",
      "0.3733925521373749\n",
      "23/23 [==============================] - 0s 591us/step - loss: 0.4022\n",
      "0.4021663963794708\n"
     ]
    }
   ],
   "source": [
    "print(model_NN.evaluate(X_test, y_test))\n",
    "print(model_NN.evaluate(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c0d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 409us/step\n",
      "6/6 [==============================] - 0s 601us/step\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the prediction for each set. Using function (pred_output) to unpack the result\n",
    "output_train = fm.pred_output(model_NN.predict(X_train))\n",
    "output_test = fm.pred_output(model_NN.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c637a82",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3cbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 83.684951\n",
      "Test Accuracy: 83.146067\n"
     ]
    }
   ],
   "source": [
    "#Printing the model accuracy\n",
    "print('Train Accuracy: %f'%(np.mean(output_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(output_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1681e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.163\n"
     ]
    }
   ],
   "source": [
    "#Checking the model error (% of incorrect guesses) using function (eval_err)\n",
    "error_test = fm.eval_err(y_test, output_test)\n",
    "error_train = fm.eval_err(y_train, output_train)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ccd54ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       107\n",
      "           1       0.86      0.69      0.77        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.83      0.83      0.83       178\n",
      "\n",
      "[[99  8]\n",
      " [22 49]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, output_test))\n",
    "print(confusion_matrix(y_test, output_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaea362",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4800eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_NN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                352       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 897\n",
      "Trainable params: 897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_NN.summary()\n",
    "model_NN.save('Model_tf_NN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b011009",
   "metadata": {},
   "source": [
    "## Building a Deep NN\n",
    "##### Models must be identical\n",
    "Since the built model is a forward propagation one, the weights must be calculated beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "017a25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create code that runs per layer:\n",
    "    #Every node uses its weights and np.dots(x, w) + b // Uses the result in the sigmoid function\n",
    "    #Each layer output is equal to the node prediction\n",
    "    #Layer input takes the last layer's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a07a2",
   "metadata": {},
   "source": [
    "### Getting the tf imported models layers weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5140b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "[l1, l2, l3] = model_NN.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e9cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpacking weights and biases\n",
    "W1_tmp, b1_tmp = l1.get_weights()\n",
    "W2_tmp, b2_tmp = l2.get_weights()\n",
    "W3_tmp, b3_tmp = l3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaf9b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32)\n",
      "(32, 16)\n",
      "(16, 1)\n",
      "(711, 10)\n",
      "(178, 10)\n"
     ]
    }
   ],
   "source": [
    "#Just a shape visualization (every output == next input)\n",
    "print(W1_tmp.shape)\n",
    "print(W2_tmp.shape)\n",
    "print(W3_tmp.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a229b",
   "metadata": {},
   "source": [
    "### Layer function\n",
    "#### Takes the weights, biases, activation function (sigmoid) and calculates the prediction for each node\n",
    "Considering the input (initially the training data), the layer uses np.dot to multiple the respective input values for the weights, inserts in the activation function and returns the prediction for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "326691a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(a_in, W, b, g):\n",
    "    #Use the input a_in\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros(units)\n",
    "    \n",
    "    for i in range(units):\n",
    "        #Activation function input\n",
    "        res = np.dot(W[:,i], a_in) + b[i]\n",
    "        a_out[i] = g(res)\n",
    "        \n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51a19d",
   "metadata": {},
   "source": [
    "### Sequential \n",
    "#### Creates the \"network\"\n",
    "Uses the layers function to get each layer output and propagate it to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7684e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential(x, W1, b1, W2, b2, W3, b3, g):\n",
    "    #Initial input value is the training/test set\n",
    "    a1 = layers(x, W1, b1, g)\n",
    "    a2 = layers(a1, W2, b2, g)\n",
    "    a3 = layers(a2, W3, b3, g)\n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce31f7",
   "metadata": {},
   "source": [
    "### Runing the NN built model\n",
    "#####  Iterates over input items returning the prediction and setting the prediction to a 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64685435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn_built(X, threshold):\n",
    "    ## Get No examples to run prediction\n",
    "    examples = X.shape[0]\n",
    "    prediction_nn_built = np.zeros(examples)\n",
    "    \n",
    "    for i in range(examples):\n",
    "        ## Iterate over all examples and add it to prediction\n",
    "        prob = sequential(X[i], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp, g=fm.sigmoid)\n",
    "        \n",
    "        #Takes the prediction as a 1/0 output\n",
    "        if prob >= threshold:\n",
    "            prob = 1\n",
    "        else:\n",
    "            prob = 0\n",
    "            \n",
    "        prediction_nn_built[i] = prob\n",
    "        \n",
    "    return prediction_nn_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dcbb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs model in bot sets\n",
    "pred_nn_built_train = run_nn_built(X_train, 0.5)\n",
    "pred_nn_built_test = run_nn_built(X_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea687a",
   "metadata": {},
   "source": [
    "## NN Built Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7add5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.163\n"
     ]
    }
   ],
   "source": [
    "#Error % in each set\n",
    "error_test = fm.eval_err(y_test, pred_nn_built_test)\n",
    "error_train = fm.eval_err(y_train, pred_nn_built_train)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23527b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 83.684951\n",
      "Test Accuracy: 83.146067\n"
     ]
    }
   ],
   "source": [
    "#Model accuracy per set\n",
    "print('Train Accuracy: %f'%(np.mean(pred_nn_built_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(pred_nn_built_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e09920",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       107\n",
      "           1       0.86      0.69      0.77        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.83      0.83      0.83       178\n",
      "\n",
      "[[99  8]\n",
      " [22 49]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_nn_built_test))\n",
    "print(confusion_matrix(y_test, pred_nn_built_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27387e6",
   "metadata": {},
   "source": [
    "## Improving the calculation time using matmul instead of np.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32a952c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_matmul(a_in, W, b, g):\n",
    "    #Use the input a_in\n",
    "    #Instead of a np.dot the z (activation function input) uses matmul to reduce the calculation demands\n",
    "    res = np.matmul(a_in, W) + b\n",
    "    a_out[i] = g(res)\n",
    "        \n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b6704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_matmul(x, W1, b1, W2, b2, W3, b3, g):\n",
    "    #Same logic as the above, changes the function call only\n",
    "    a1 = layers_matmul(x, W1, b1, g)\n",
    "    a2 = layers_matmul(a1, W2, b2, g)\n",
    "    a3 = layers_matmul(a2, W3, b3, g)\n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a0f5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn_built_matmul(X, threshold):\n",
    "    ## Get No examples to run prediction\n",
    "    examples = X.shape[0]\n",
    "    prediction_nn_built = np.zeros(examples)\n",
    "    \n",
    "    for i in range(examples):\n",
    "        ## Iterate over all examples and add it to prediction\n",
    "        prob = sequential_matmul(X[i], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp, g=fm.sigmoid)\n",
    "        \n",
    "        if prob >= threshold:\n",
    "            prob = 1\n",
    "        else:\n",
    "            prob = 0\n",
    "            \n",
    "        prediction_nn_built[i] = prob\n",
    "        \n",
    "    return prediction_nn_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12123bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn_built_train_matmul = run_nn_built(X_train, 0.5)\n",
    "pred_nn_built_test_matmul = run_nn_built(X_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f18dd",
   "metadata": {},
   "source": [
    "## NN Built Matmul Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "415490a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.163\n"
     ]
    }
   ],
   "source": [
    "error_test = fm.eval_err(y_test, pred_nn_built_test_matmul)\n",
    "error_train = fm.eval_err(y_train, pred_nn_built_train_matmul)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9558e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.146067\n",
      "Train Accuracy: 83.684951\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %f'%(np.mean(pred_nn_built_test_matmul == y_test) * 100))\n",
    "print('Train Accuracy: %f'%(np.mean(pred_nn_built_train_matmul == y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7feae30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.93      0.87       107\n",
      "           1       0.86      0.69      0.77        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.83      0.83      0.83       178\n",
      "\n",
      "[[99  8]\n",
      " [22 49]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_nn_built_test_matmul))\n",
    "print(confusion_matrix(y_test, pred_nn_built_test_matmul))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
