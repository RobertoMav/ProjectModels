{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec29bd8a",
   "metadata": {},
   "source": [
    "# NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "348cfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b2e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b604d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_pred(yhat):\n",
    "    if yhat >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74244782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_err(y, yhat):\n",
    "    m = y.shape[0]\n",
    "    incorrect = 0\n",
    "    y = y.tolist()\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:\n",
    "            incorrect += 1\n",
    "            \n",
    "    incorrect = incorrect / m\n",
    "    \n",
    "    return incorrect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1456bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_output(prediction):\n",
    "    ex = prediction.shape[0]\n",
    "    output = []\n",
    "    for i in range(ex):\n",
    "        output.append(NN_pred(prediction[i]))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52f4dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    calc = math.e**-z\n",
    "    g = 1 / (1 + calc)\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c48d28ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>AgeFare</th>\n",
       "      <th>SibPar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>159.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2708.7654</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>206.0500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1858.5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>281.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Q  S    AgeFare  SibPar\n",
       "0         0       3    1  22.0      1      0   7.2500  0  1   159.5000       1\n",
       "1         1       1    0  38.0      1      0  71.2833  0  0  2708.7654       1\n",
       "2         1       3    0  26.0      0      0   7.9250  0  1   206.0500       0\n",
       "3         1       1    0  35.0      1      0  53.1000  0  1  1858.5000       1\n",
       "4         0       3    1  35.0      0      0   8.0500  0  1   281.7500       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = pd.read_csv('TrainSet1.csv')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38b1781",
   "metadata": {},
   "source": [
    "### Creating x_train, y_train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_set['Survived']\n",
    "x = train_set.drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e42fc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af203c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd565bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train, y_train)\n",
    "X_test = scaler.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d453d6",
   "metadata": {},
   "source": [
    "### Creating a TF-Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77fac65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the sigmoid activation function to get the best weights and biases for the created model (below)\n",
    "model_NN = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(10,)),\n",
    "        Dense(units=32, activation='sigmoid'),\n",
    "        Dense(units=16, activation='sigmoid'),\n",
    "        Dense(units=1, activation='sigmoid'),\n",
    "    ], name=\"model_NN\")\n",
    "\n",
    "model_NN.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b8851",
   "metadata": {},
   "source": [
    "#### Using an early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a63fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5eca1c",
   "metadata": {},
   "source": [
    "### Fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c5e62d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.7567 - val_loss: 0.6930\n",
      "Epoch 2/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6689 - val_loss: 0.6536\n",
      "Epoch 3/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6413 - val_loss: 0.6422\n",
      "Epoch 4/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6308 - val_loss: 0.6337\n",
      "Epoch 5/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6217 - val_loss: 0.6230\n",
      "Epoch 6/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6119 - val_loss: 0.6110\n",
      "Epoch 7/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6020 - val_loss: 0.5999\n",
      "Epoch 8/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5862\n",
      "Epoch 9/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5723\n",
      "Epoch 10/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5670 - val_loss: 0.5584\n",
      "Epoch 11/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5548 - val_loss: 0.5454\n",
      "Epoch 12/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5434 - val_loss: 0.5319\n",
      "Epoch 13/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5319 - val_loss: 0.5195\n",
      "Epoch 14/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5206 - val_loss: 0.5070\n",
      "Epoch 15/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5105 - val_loss: 0.4943\n",
      "Epoch 16/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.4844\n",
      "Epoch 17/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4919 - val_loss: 0.4763\n",
      "Epoch 18/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4844 - val_loss: 0.4666\n",
      "Epoch 19/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4776 - val_loss: 0.4587\n",
      "Epoch 20/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4736 - val_loss: 0.4558\n",
      "Epoch 21/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.4479\n",
      "Epoch 22/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4638 - val_loss: 0.4457\n",
      "Epoch 23/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4600 - val_loss: 0.4421\n",
      "Epoch 24/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4389\n",
      "Epoch 25/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4551 - val_loss: 0.4367\n",
      "Epoch 26/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4532 - val_loss: 0.4324\n",
      "Epoch 27/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 0.4308\n",
      "Epoch 28/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4506 - val_loss: 0.4286\n",
      "Epoch 29/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4495 - val_loss: 0.4282\n",
      "Epoch 30/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4484 - val_loss: 0.4266\n",
      "Epoch 31/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4487 - val_loss: 0.4251\n",
      "Epoch 32/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4478 - val_loss: 0.4231\n",
      "Epoch 33/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4463 - val_loss: 0.4245\n",
      "Epoch 34/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4458 - val_loss: 0.4232\n",
      "Epoch 35/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4454 - val_loss: 0.4227\n",
      "Epoch 36/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4449 - val_loss: 0.4222\n",
      "Epoch 37/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4443 - val_loss: 0.4215\n",
      "Epoch 38/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4439 - val_loss: 0.4201\n",
      "Epoch 39/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4190\n",
      "Epoch 40/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4431 - val_loss: 0.4194\n",
      "Epoch 41/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4187\n",
      "Epoch 42/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4197\n",
      "Epoch 43/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4422 - val_loss: 0.4187\n",
      "Epoch 44/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4421 - val_loss: 0.4167\n",
      "Epoch 45/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4161\n",
      "Epoch 46/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4415 - val_loss: 0.4178\n",
      "Epoch 47/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4169\n",
      "Epoch 48/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4406 - val_loss: 0.4158\n",
      "Epoch 49/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4407 - val_loss: 0.4170\n",
      "Epoch 50/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4158\n",
      "Epoch 51/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4398 - val_loss: 0.4163\n",
      "Epoch 52/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4145\n",
      "Epoch 53/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4402 - val_loss: 0.4148\n",
      "Epoch 54/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4405 - val_loss: 0.4146\n",
      "Epoch 55/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4165\n",
      "Epoch 56/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4139\n",
      "Epoch 57/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4135\n",
      "Epoch 58/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.4147\n",
      "Epoch 59/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4380 - val_loss: 0.4146\n",
      "Epoch 60/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4131\n",
      "Epoch 61/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4378 - val_loss: 0.4127\n",
      "Epoch 62/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4377 - val_loss: 0.4117\n",
      "Epoch 63/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4371 - val_loss: 0.4125\n",
      "Epoch 64/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4116\n",
      "Epoch 65/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4109\n",
      "Epoch 66/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4368 - val_loss: 0.4101\n",
      "Epoch 67/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4110\n",
      "Epoch 68/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4112\n",
      "Epoch 69/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4363 - val_loss: 0.4111\n",
      "Epoch 70/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4360 - val_loss: 0.4116\n",
      "Epoch 71/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4357 - val_loss: 0.4110\n",
      "Epoch 72/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4362 - val_loss: 0.4108\n",
      "Epoch 73/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4080\n",
      "Epoch 74/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4093\n",
      "Epoch 75/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4354 - val_loss: 0.4106\n",
      "Epoch 76/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4096\n",
      "Epoch 77/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4083\n",
      "Epoch 78/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4086\n",
      "Epoch 79/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4098\n",
      "Epoch 80/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4346 - val_loss: 0.4095\n",
      "Epoch 81/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4349 - val_loss: 0.4092\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4345 - val_loss: 0.4079\n",
      "Epoch 83/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4103\n",
      "Epoch 84/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4088\n",
      "Epoch 85/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4092\n",
      "Epoch 86/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4087\n",
      "Epoch 87/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4338 - val_loss: 0.4101\n",
      "Epoch 88/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4336 - val_loss: 0.4084\n",
      "Epoch 89/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4333 - val_loss: 0.4078\n",
      "Epoch 90/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4331 - val_loss: 0.4062\n",
      "Epoch 91/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4332 - val_loss: 0.4077\n",
      "Epoch 92/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4067\n",
      "Epoch 93/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4347 - val_loss: 0.4063\n",
      "Epoch 94/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4324 - val_loss: 0.4075\n",
      "Epoch 95/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4326 - val_loss: 0.4068\n",
      "Epoch 96/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4064\n",
      "Epoch 97/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - val_loss: 0.4065\n",
      "Epoch 98/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4323 - val_loss: 0.4070\n",
      "Epoch 99/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4063\n",
      "Epoch 100/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4325 - val_loss: 0.4088\n",
      "Epoch 101/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4065\n",
      "Epoch 102/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4318 - val_loss: 0.4074\n",
      "Epoch 103/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4058\n",
      "Epoch 104/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4314 - val_loss: 0.4065\n",
      "Epoch 105/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4057\n",
      "Epoch 106/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4065\n",
      "Epoch 107/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4317 - val_loss: 0.4056\n",
      "Epoch 108/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4316 - val_loss: 0.4066\n",
      "Epoch 109/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4047\n",
      "Epoch 110/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4053\n",
      "Epoch 111/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4048\n",
      "Epoch 112/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4311 - val_loss: 0.4048\n",
      "Epoch 113/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4315 - val_loss: 0.4070\n",
      "Epoch 114/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4040\n",
      "Epoch 115/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4303 - val_loss: 0.4044\n",
      "Epoch 116/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4305 - val_loss: 0.4051\n",
      "Epoch 117/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4035\n",
      "Epoch 118/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4029\n",
      "Epoch 119/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4308 - val_loss: 0.4024\n",
      "Epoch 120/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4025\n",
      "Epoch 121/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4298 - val_loss: 0.4027\n",
      "Epoch 122/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.4035\n",
      "Epoch 123/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4013\n",
      "Epoch 124/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4300 - val_loss: 0.4040\n",
      "Epoch 125/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4291 - val_loss: 0.4016\n",
      "Epoch 126/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4003\n",
      "Epoch 127/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4295 - val_loss: 0.3999\n",
      "Epoch 128/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4294 - val_loss: 0.4014\n",
      "Epoch 129/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4021\n",
      "Epoch 130/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4293 - val_loss: 0.4020\n",
      "Epoch 131/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4290 - val_loss: 0.4010\n",
      "Epoch 132/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4006\n",
      "Epoch 133/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4012\n",
      "Epoch 134/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4004\n",
      "Epoch 135/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4286 - val_loss: 0.3990\n",
      "Epoch 136/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4003\n",
      "Epoch 137/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.3996\n",
      "Epoch 138/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4280 - val_loss: 0.4004\n",
      "Epoch 139/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4281 - val_loss: 0.4013\n",
      "Epoch 140/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4285 - val_loss: 0.4005\n",
      "Epoch 141/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4283 - val_loss: 0.3996\n",
      "Epoch 142/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.3999\n",
      "Epoch 143/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4278 - val_loss: 0.4006\n",
      "Epoch 144/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4279 - val_loss: 0.4006\n",
      "Epoch 145/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.3993\n",
      "Epoch 146/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.3988\n",
      "Epoch 147/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.3992\n",
      "Epoch 148/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3975\n",
      "Epoch 149/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4276 - val_loss: 0.3989\n",
      "Epoch 150/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.3971\n",
      "Epoch 151/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4274 - val_loss: 0.3990\n",
      "Epoch 152/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4270 - val_loss: 0.3996\n",
      "Epoch 153/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.3990\n",
      "Epoch 154/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.3982\n",
      "Epoch 155/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.3979\n",
      "Epoch 156/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3975\n",
      "Epoch 157/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3971\n",
      "Epoch 158/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.3971\n",
      "Epoch 159/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4269 - val_loss: 0.3967\n",
      "Epoch 160/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4266 - val_loss: 0.3976\n",
      "Epoch 161/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4264 - val_loss: 0.3980\n",
      "Epoch 162/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4265 - val_loss: 0.3966\n",
      "Epoch 163/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3972\n",
      "Epoch 164/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3976\n",
      "Epoch 165/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4268 - val_loss: 0.3966\n",
      "Epoch 166/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4262 - val_loss: 0.3988\n",
      "Epoch 167/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.3967\n",
      "Epoch 168/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4260 - val_loss: 0.3989\n",
      "Epoch 169/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3968\n",
      "Epoch 170/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.3978\n",
      "Epoch 171/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.3954\n",
      "Epoch 172/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.3953\n",
      "Epoch 173/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4256 - val_loss: 0.3953\n",
      "Epoch 174/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.3959\n",
      "Epoch 175/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4254 - val_loss: 0.3948\n",
      "Epoch 176/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4252 - val_loss: 0.3951\n",
      "Epoch 177/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4255 - val_loss: 0.3948\n",
      "Epoch 178/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.3944\n",
      "Epoch 179/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.3964\n",
      "Epoch 180/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.3943\n",
      "Epoch 181/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4249 - val_loss: 0.3949\n",
      "Epoch 182/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4253 - val_loss: 0.3943\n",
      "Epoch 183/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.3937\n",
      "Epoch 184/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3951\n",
      "Epoch 185/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.3943\n",
      "Epoch 186/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.3935\n",
      "Epoch 187/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3937\n",
      "Epoch 188/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4244 - val_loss: 0.3930\n",
      "Epoch 189/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.3929\n",
      "Epoch 190/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4242 - val_loss: 0.3938\n",
      "Epoch 191/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4240 - val_loss: 0.3934\n",
      "Epoch 192/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4241 - val_loss: 0.3931\n",
      "Epoch 193/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.3924\n",
      "Epoch 194/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4237 - val_loss: 0.3917\n",
      "Epoch 195/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4245 - val_loss: 0.3916\n",
      "Epoch 196/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4234 - val_loss: 0.3933\n",
      "Epoch 197/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.3939\n",
      "Epoch 198/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4239 - val_loss: 0.3915\n",
      "Epoch 199/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.3924\n",
      "Epoch 200/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.3934\n",
      "Epoch 201/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.3921\n",
      "Epoch 202/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4233 - val_loss: 0.3913\n",
      "Epoch 203/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4230 - val_loss: 0.3933\n",
      "Epoch 204/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4228 - val_loss: 0.3942\n",
      "Epoch 205/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.3933\n",
      "Epoch 206/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.3930\n",
      "Epoch 207/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4232 - val_loss: 0.3929\n",
      "Epoch 208/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4236 - val_loss: 0.3917\n",
      "Epoch 209/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.3920\n",
      "Epoch 210/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.3904\n",
      "Epoch 211/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4231 - val_loss: 0.3896\n",
      "Epoch 212/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4226 - val_loss: 0.3928\n",
      "Epoch 213/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4220 - val_loss: 0.3904\n",
      "Epoch 214/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4218 - val_loss: 0.3913\n",
      "Epoch 215/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.3926\n",
      "Epoch 216/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.3915\n",
      "Epoch 217/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4221 - val_loss: 0.3915\n",
      "Epoch 218/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.3907\n",
      "Epoch 219/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4216 - val_loss: 0.3909\n",
      "Epoch 220/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.3906\n",
      "Epoch 221/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.3893\n",
      "Epoch 222/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4214 - val_loss: 0.3885\n",
      "Epoch 223/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4212 - val_loss: 0.3886\n",
      "Epoch 224/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4210 - val_loss: 0.3883\n",
      "Epoch 225/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.3908\n",
      "Epoch 226/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.3912\n",
      "Epoch 227/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4207 - val_loss: 0.3909\n",
      "Epoch 228/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.3893\n",
      "Epoch 229/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3902\n",
      "Epoch 230/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.3905\n",
      "Epoch 231/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.3897\n",
      "Epoch 232/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.3887\n",
      "Epoch 233/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.3884\n",
      "Epoch 234/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.3898\n",
      "Epoch 235/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4199 - val_loss: 0.3889\n",
      "Epoch 236/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.3879\n",
      "Epoch 237/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.3891\n",
      "Epoch 238/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4200 - val_loss: 0.3893\n",
      "Epoch 239/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4202 - val_loss: 0.3878\n",
      "Epoch 240/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.3900\n",
      "Epoch 241/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4196 - val_loss: 0.3876\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.3873\n",
      "Epoch 243/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4191 - val_loss: 0.3879\n",
      "Epoch 244/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.3857\n",
      "Epoch 245/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4186 - val_loss: 0.3876\n",
      "Epoch 246/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4189 - val_loss: 0.3883\n",
      "Epoch 247/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4195 - val_loss: 0.3879\n",
      "Epoch 248/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.3850\n",
      "Epoch 249/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4185 - val_loss: 0.3859\n",
      "Epoch 250/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.3861\n",
      "Epoch 251/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4190 - val_loss: 0.3863\n",
      "Epoch 252/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4187 - val_loss: 0.3865\n",
      "Epoch 253/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4178 - val_loss: 0.3843\n",
      "Epoch 254/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3863\n",
      "Epoch 255/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3869\n",
      "Epoch 256/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3863\n",
      "Epoch 257/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4176 - val_loss: 0.3842\n",
      "Epoch 258/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.3850\n",
      "Epoch 259/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4177 - val_loss: 0.3847\n",
      "Epoch 260/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.3860\n",
      "Epoch 261/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.3833\n",
      "Epoch 262/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.3837\n",
      "Epoch 263/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4174 - val_loss: 0.3831\n",
      "Epoch 264/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4170 - val_loss: 0.3855\n",
      "Epoch 265/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4172 - val_loss: 0.3853\n",
      "Epoch 266/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.3839\n",
      "Epoch 267/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4169 - val_loss: 0.3852\n",
      "Epoch 268/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.3838\n",
      "Epoch 269/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4166 - val_loss: 0.3817\n",
      "Epoch 270/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4162 - val_loss: 0.3826\n",
      "Epoch 271/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3823\n",
      "Epoch 272/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.3838\n",
      "Epoch 273/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4158 - val_loss: 0.3822\n",
      "Epoch 274/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4155 - val_loss: 0.3826\n",
      "Epoch 275/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.3828\n",
      "Epoch 276/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4157 - val_loss: 0.3811\n",
      "Epoch 277/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3814\n",
      "Epoch 278/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.3804\n",
      "Epoch 279/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.3819\n",
      "Epoch 280/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3825\n",
      "Epoch 281/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4150 - val_loss: 0.3834\n",
      "Epoch 282/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3812\n",
      "Epoch 283/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4145 - val_loss: 0.3804\n",
      "Epoch 284/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4151 - val_loss: 0.3809\n",
      "Epoch 285/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3796\n",
      "Epoch 286/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.3799\n",
      "Epoch 287/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4142 - val_loss: 0.3815\n",
      "Epoch 288/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4146 - val_loss: 0.3807\n",
      "Epoch 289/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4147 - val_loss: 0.3805\n",
      "Epoch 290/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4140 - val_loss: 0.3812\n",
      "Epoch 291/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.3806\n",
      "Epoch 292/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4138 - val_loss: 0.3816\n",
      "Epoch 293/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.3820\n",
      "Epoch 294/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4137 - val_loss: 0.3811\n",
      "Epoch 295/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4148 - val_loss: 0.3847\n",
      "Epoch 296/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4133 - val_loss: 0.3798\n",
      "Epoch 297/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3811\n",
      "Epoch 298/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.3806\n",
      "Epoch 299/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.3812\n",
      "Epoch 300/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.3801\n",
      "Epoch 301/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4131 - val_loss: 0.3807\n",
      "Epoch 302/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4129 - val_loss: 0.3817\n",
      "Epoch 303/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4125 - val_loss: 0.3812\n",
      "Epoch 304/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.3794\n",
      "Epoch 305/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3783\n",
      "Epoch 306/1000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4121 - val_loss: 0.3787\n",
      "Epoch 307/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.3803\n",
      "Epoch 308/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.3792\n",
      "Epoch 309/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4117 - val_loss: 0.3789\n",
      "Epoch 310/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4116 - val_loss: 0.3786\n",
      "Epoch 311/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.3794\n",
      "Epoch 312/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4110 - val_loss: 0.3801\n",
      "Epoch 313/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4115 - val_loss: 0.3781\n",
      "Epoch 314/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.3799\n",
      "Epoch 315/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4107 - val_loss: 0.3798\n",
      "Epoch 316/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3795\n",
      "Epoch 317/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4106 - val_loss: 0.3789\n",
      "Epoch 318/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4113 - val_loss: 0.3792\n",
      "Epoch 319/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.3768\n",
      "Epoch 320/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4111 - val_loss: 0.3789\n",
      "Epoch 321/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3778\n",
      "Epoch 322/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4102 - val_loss: 0.3762\n",
      "Epoch 323/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4100 - val_loss: 0.3775\n",
      "Epoch 324/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4097 - val_loss: 0.3760\n",
      "Epoch 325/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3776\n",
      "Epoch 326/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 0.3760\n",
      "Epoch 327/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.3771\n",
      "Epoch 328/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.3760\n",
      "Epoch 329/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4090 - val_loss: 0.3766\n",
      "Epoch 330/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4088 - val_loss: 0.3768\n",
      "Epoch 331/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.3774\n",
      "Epoch 332/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4085 - val_loss: 0.3777\n",
      "Epoch 333/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4089 - val_loss: 0.3765\n",
      "Epoch 334/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.3774\n",
      "Epoch 335/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4081 - val_loss: 0.3756\n",
      "Epoch 336/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4076 - val_loss: 0.3768\n",
      "Epoch 337/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.3760\n",
      "Epoch 338/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.3752\n",
      "Epoch 339/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4084 - val_loss: 0.3737\n",
      "Epoch 340/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4086 - val_loss: 0.3758\n",
      "Epoch 341/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4077 - val_loss: 0.3736\n",
      "Epoch 342/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.3752\n",
      "Epoch 343/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.3740\n",
      "Epoch 344/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4068 - val_loss: 0.3747\n",
      "Epoch 345/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4070 - val_loss: 0.3738\n",
      "Epoch 346/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4069 - val_loss: 0.3747\n",
      "Epoch 347/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.3746\n",
      "Epoch 348/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3750\n",
      "Epoch 349/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3743\n",
      "Epoch 350/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3743\n",
      "Epoch 351/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4065 - val_loss: 0.3728\n",
      "Epoch 352/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4060 - val_loss: 0.3749\n",
      "Epoch 353/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.3752\n",
      "Epoch 354/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.3727\n",
      "Epoch 355/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.3738\n",
      "Epoch 356/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.3724\n",
      "Epoch 357/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4054 - val_loss: 0.3725\n",
      "Epoch 358/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.3715\n",
      "Epoch 359/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4052 - val_loss: 0.3719\n",
      "Epoch 360/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.3724\n",
      "Epoch 361/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4050 - val_loss: 0.3744\n",
      "Epoch 362/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.3728\n",
      "Epoch 363/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4043 - val_loss: 0.3712\n",
      "Epoch 364/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.3720\n",
      "Epoch 365/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.3723\n",
      "Epoch 366/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.3743\n",
      "Epoch 367/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4039 - val_loss: 0.3738\n",
      "Epoch 368/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.3715\n",
      "Epoch 369/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.3719\n",
      "Epoch 370/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.3718\n",
      "Epoch 371/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.3739\n",
      "Epoch 372/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.3727\n",
      "Epoch 373/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.3717\n",
      "Epoch 374/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4040 - val_loss: 0.3721\n",
      "Epoch 375/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4031 - val_loss: 0.3740\n",
      "Epoch 376/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.3703\n",
      "Epoch 377/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.3719\n",
      "Epoch 378/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3720\n",
      "Epoch 379/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4026 - val_loss: 0.3720\n",
      "Epoch 380/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4024 - val_loss: 0.3717\n",
      "Epoch 381/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.3717\n",
      "Epoch 382/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.3713\n",
      "Epoch 383/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4025 - val_loss: 0.3726\n",
      "Epoch 384/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4028 - val_loss: 0.3695\n",
      "Epoch 385/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4018 - val_loss: 0.3713\n",
      "Epoch 386/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.3702\n",
      "Epoch 387/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4011 - val_loss: 0.3698\n",
      "Epoch 388/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4013 - val_loss: 0.3711\n",
      "Epoch 389/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4009 - val_loss: 0.3708\n",
      "Epoch 390/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.3725\n",
      "Epoch 391/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.3693\n",
      "Epoch 392/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4007 - val_loss: 0.3695\n",
      "Epoch 393/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4014 - val_loss: 0.3695\n",
      "Epoch 394/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.3708\n",
      "Epoch 395/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.3700\n",
      "Epoch 396/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3998 - val_loss: 0.3713\n",
      "Epoch 397/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4003 - val_loss: 0.3707\n",
      "Epoch 398/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 0.3690\n",
      "Epoch 399/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4004 - val_loss: 0.3701\n",
      "Epoch 400/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3999 - val_loss: 0.3703\n",
      "Epoch 401/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4000 - val_loss: 0.3689\n",
      "Epoch 402/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.3698\n",
      "Epoch 403/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.3703\n",
      "Epoch 404/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3990 - val_loss: 0.3701\n",
      "Epoch 405/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3993 - val_loss: 0.3693\n",
      "Epoch 406/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.3715\n",
      "Epoch 407/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.3689\n",
      "Epoch 408/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3980 - val_loss: 0.3700\n",
      "Epoch 409/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.3708\n",
      "Epoch 410/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3683\n",
      "Epoch 411/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.3691\n",
      "Epoch 412/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3992 - val_loss: 0.3690\n",
      "Epoch 413/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.3682\n",
      "Epoch 414/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3976 - val_loss: 0.3684\n",
      "Epoch 415/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3978 - val_loss: 0.3679\n",
      "Epoch 416/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3977 - val_loss: 0.3686\n",
      "Epoch 417/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.3692\n",
      "Epoch 418/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.3686\n",
      "Epoch 419/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3987 - val_loss: 0.3699\n",
      "Epoch 420/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3974 - val_loss: 0.3671\n",
      "Epoch 421/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3971 - val_loss: 0.3689\n",
      "Epoch 422/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3972 - val_loss: 0.3690\n",
      "Epoch 423/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3970 - val_loss: 0.3678\n",
      "Epoch 424/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.3690\n",
      "Epoch 425/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.3681\n",
      "Epoch 426/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.3691\n",
      "Epoch 427/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3969 - val_loss: 0.3690\n",
      "Epoch 428/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3970 - val_loss: 0.3665\n",
      "Epoch 429/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3970 - val_loss: 0.3682\n",
      "Epoch 430/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3690\n",
      "Epoch 431/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.3682\n",
      "Epoch 432/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.3671\n",
      "Epoch 433/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3962 - val_loss: 0.3711\n",
      "Epoch 434/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.3687\n",
      "Epoch 435/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3967 - val_loss: 0.3676\n",
      "Epoch 436/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3966 - val_loss: 0.3695\n",
      "Epoch 437/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3682\n",
      "Epoch 438/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3954 - val_loss: 0.3662\n",
      "Epoch 439/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3687\n",
      "Epoch 440/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3952 - val_loss: 0.3690\n",
      "Epoch 441/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3674\n",
      "Epoch 442/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3652\n",
      "Epoch 443/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3947 - val_loss: 0.3673\n",
      "Epoch 444/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3948 - val_loss: 0.3663\n",
      "Epoch 445/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3674\n",
      "Epoch 446/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3684\n",
      "Epoch 447/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3951 - val_loss: 0.3670\n",
      "Epoch 448/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.3672\n",
      "Epoch 449/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3938 - val_loss: 0.3677\n",
      "Epoch 450/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3940 - val_loss: 0.3684\n",
      "Epoch 451/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3934 - val_loss: 0.3668\n",
      "Epoch 452/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.3670\n",
      "Epoch 453/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3692\n",
      "Epoch 454/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.3684\n",
      "Epoch 455/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.3675\n",
      "Epoch 456/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.3679\n",
      "Epoch 457/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.3682\n",
      "Epoch 458/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.3668\n",
      "Epoch 459/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3939 - val_loss: 0.3686\n",
      "Epoch 460/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3928 - val_loss: 0.3658\n",
      "Epoch 461/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3930 - val_loss: 0.3661\n",
      "Epoch 462/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3672\n",
      "Epoch 463/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.3680\n",
      "Epoch 464/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3919 - val_loss: 0.3669\n",
      "Epoch 465/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3922 - val_loss: 0.3675\n",
      "Epoch 466/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3918 - val_loss: 0.3665\n",
      "Epoch 467/1000\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3921 - val_loss: 0.3673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2d2ccef10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the model\n",
    "model_NN.fit(x=X_train, y=y_train, epochs=1000, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b0b0f",
   "metadata": {},
   "source": [
    "### Plotting the loss metrics vs epoch history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25e92aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_mol = pd.DataFrame(model_NN.history.history)\n",
    "loss_mol.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2753d203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "[[0.9904069]]\n"
     ]
    }
   ],
   "source": [
    "#Checking the 1st train_set prediction (y = 0)\n",
    "print(model_NN.predict(X_train[0].reshape(1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c68a58",
   "metadata": {},
   "source": [
    "### Obtaining the model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3996aabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 601us/step - loss: 0.3673\n",
      "0.3673446774482727\n",
      "23/23 [==============================] - 0s 546us/step - loss: 0.3911\n",
      "0.391137957572937\n"
     ]
    }
   ],
   "source": [
    "print(model_NN.evaluate(X_test, y_test))\n",
    "print(model_NN.evaluate(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61c0d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 409us/step\n",
      "6/6 [==============================] - 0s 600us/step\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the prediction for each set. Using function (pred_output) to unpack the result\n",
    "output_train = pred_output(model_NN.predict(X_train))\n",
    "output_test = pred_output(model_NN.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c637a82",
   "metadata": {},
   "source": [
    "## Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f3cbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.247539\n",
      "Test Accuracy: 83.146067\n"
     ]
    }
   ],
   "source": [
    "#Printing the model accuracy\n",
    "print('Train Accuracy: %f'%(np.mean(output_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(output_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1681e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.158\n"
     ]
    }
   ],
   "source": [
    "#Checking the model error (% of incorrect guesses) using function (eval_err)\n",
    "error_test = eval_err(y_test, output_test)\n",
    "error_train = eval_err(y_train, output_train)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ccd54ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       107\n",
      "           1       0.87      0.68      0.76        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.84      0.83      0.83       178\n",
      "\n",
      "[[100   7]\n",
      " [ 23  48]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, output_test))\n",
    "print(confusion_matrix(y_test, output_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaea362",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4800eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_NN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 32)                352       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 897\n",
      "Trainable params: 897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_NN.summary()\n",
    "model_NN.save('Model_tf_NN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b011009",
   "metadata": {},
   "source": [
    "## Building a Deep NN\n",
    "##### Models must be identical\n",
    "Since the built model is a forward propagation one, the weights must be calculated beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "017a25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create code that runs per layer:\n",
    "    #Every node uses its weights and np.dots(x, w) + b // Uses the result in the sigmoid function\n",
    "    #Each layer output is equal to the node prediction\n",
    "    #Layer input takes the last layer's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a07a2",
   "metadata": {},
   "source": [
    "### Getting the tf imported models layers weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5140b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "[l1, l2, l3] = model_NN.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24e9cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpacking weights and biases\n",
    "W1_tmp, b1_tmp = l1.get_weights()\n",
    "W2_tmp, b2_tmp = l2.get_weights()\n",
    "W3_tmp, b3_tmp = l3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaf9b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32)\n",
      "(32, 16)\n",
      "(16, 1)\n",
      "(711, 10)\n",
      "(178, 10)\n"
     ]
    }
   ],
   "source": [
    "#Just a shape visualization (every output == next input)\n",
    "print(W1_tmp.shape)\n",
    "print(W2_tmp.shape)\n",
    "print(W3_tmp.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a229b",
   "metadata": {},
   "source": [
    "### Layer function\n",
    "#### Takes the weights, biases, activation function (sigmoid) and calculates the prediction for each node\n",
    "Considering the input (initially the training data), the layer uses np.dot to multiple the respective input values for the weights, inserts in the activation function and returns the prediction for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "326691a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(a_in, W, b, g):\n",
    "    #Use the input a_in\n",
    "    units = W.shape[1]\n",
    "    a_out = np.zeros(units)\n",
    "    \n",
    "    for i in range(units):\n",
    "        #Activation function input\n",
    "        res = np.dot(W[:,i], a_in) + b[i]\n",
    "        a_out[i] = g(res)\n",
    "        \n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51a19d",
   "metadata": {},
   "source": [
    "### Sequential \n",
    "#### Creates the \"network\"\n",
    "Uses the layers function to get each layer output and propagate it to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7684e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential(x, W1, b1, W2, b2, W3, b3, g):\n",
    "    #Initial input value is the training/test set\n",
    "    a1 = layers(x, W1, b1, g)\n",
    "    a2 = layers(a1, W2, b2, g)\n",
    "    a3 = layers(a2, W3, b3, g)\n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce31f7",
   "metadata": {},
   "source": [
    "### Runing the NN built model\n",
    "#####  Iterates over input items returning the prediction and setting the prediction to a 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "64685435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn_built(X, threshold):\n",
    "    ## Get No examples to run prediction\n",
    "    examples = X.shape[0]\n",
    "    prediction_nn_built = np.zeros(examples)\n",
    "    \n",
    "    for i in range(examples):\n",
    "        ## Iterate over all examples and add it to prediction\n",
    "        prob = sequential(X[i], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp, g=sigmoid)\n",
    "        \n",
    "        #Takes the prediction as a 1/0 output\n",
    "        if prob >= threshold:\n",
    "            prob = 1\n",
    "        else:\n",
    "            prob = 0\n",
    "            \n",
    "        prediction_nn_built[i] = prob\n",
    "        \n",
    "    return prediction_nn_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dcbb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs model in bot sets\n",
    "pred_nn_built_train = run_nn_built(X_train, 0.5)\n",
    "pred_nn_built_test = run_nn_built(X_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea687a",
   "metadata": {},
   "source": [
    "## NN Built Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab7add5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.158\n"
     ]
    }
   ],
   "source": [
    "#Error % in each set\n",
    "error_test = eval_err(y_test, pred_nn_built_test)\n",
    "error_train = eval_err(y_train, pred_nn_built_train)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23527b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.247539\n",
      "Test Accuracy: 83.146067\n"
     ]
    }
   ],
   "source": [
    "#Model accuracy per set\n",
    "print('Train Accuracy: %f'%(np.mean(pred_nn_built_train == y_train) * 100))\n",
    "print('Test Accuracy: %f'%(np.mean(pred_nn_built_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25e09920",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       107\n",
      "           1       0.87      0.68      0.76        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.84      0.83      0.83       178\n",
      "\n",
      "[[100   7]\n",
      " [ 23  48]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_nn_built_test))\n",
    "print(confusion_matrix(y_test, pred_nn_built_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27387e6",
   "metadata": {},
   "source": [
    "## Improving the calculation time using matmul instead of np.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32a952c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_matmul(a_in, W, b, g):\n",
    "    #Use the input a_in\n",
    "    #Instead of a np.dot the z (activation function input) uses matmul to reduce the calculation demands\n",
    "    res = np.matmul(a_in, W) + b\n",
    "    a_out[i] = g(res)\n",
    "        \n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b6704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_matmul(x, W1, b1, W2, b2, W3, b3, g):\n",
    "    #Same logic as the above, changes the function call only\n",
    "    a1 = layers_matmul(x, W1, b1, g)\n",
    "    a2 = layers_matmul(a1, W2, b2, g)\n",
    "    a3 = layers_matmul(a2, W3, b3, g)\n",
    "    \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a0f5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nn_built_matmul(X, threshold):\n",
    "    ## Get No examples to run prediction\n",
    "    examples = X.shape[0]\n",
    "    prediction_nn_built = np.zeros(examples)\n",
    "    \n",
    "    for i in range(examples):\n",
    "        ## Iterate over all examples and add it to prediction\n",
    "        prob = sequential_matmul(X[i], W1_tmp, b1_tmp, W2_tmp, b2_tmp, W3_tmp, b3_tmp, g=sigmoid)\n",
    "        \n",
    "        if prob >= threshold:\n",
    "            prob = 1\n",
    "        else:\n",
    "            prob = 0\n",
    "            \n",
    "        prediction_nn_built[i] = prob\n",
    "        \n",
    "    return prediction_nn_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12123bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn_built_train_matmul = run_nn_built(X_train, 0.5)\n",
    "pred_nn_built_test_matmul = run_nn_built(X_test, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f18dd",
   "metadata": {},
   "source": [
    "## NN Built Matmul Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "415490a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error test:  0.169\n",
      "error train: 0.158\n"
     ]
    }
   ],
   "source": [
    "error_test = eval_err(y_test, pred_nn_built_test_matmul)\n",
    "error_train = eval_err(y_train, pred_nn_built_train_matmul)\n",
    "print(f\"error test:  {error_test :0.3f}\")\n",
    "print(f\"error train: {error_train :0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9558e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.146067\n",
      "Train Accuracy: 84.247539\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %f'%(np.mean(pred_nn_built_test_matmul == y_test) * 100))\n",
    "print('Train Accuracy: %f'%(np.mean(pred_nn_built_train_matmul == y_train) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7feae30a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       107\n",
      "           1       0.87      0.68      0.76        71\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.84      0.81      0.82       178\n",
      "weighted avg       0.84      0.83      0.83       178\n",
      "\n",
      "[[100   7]\n",
      " [ 23  48]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_nn_built_test_matmul))\n",
    "print(confusion_matrix(y_test, pred_nn_built_test_matmul))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
